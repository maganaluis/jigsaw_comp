{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('https://s3.amazonaws.com/ccwf-ml-data/jigsaw/processed_test.csv')\n",
    "train_df = pd.read_csv('https://s3.amazonaws.com/ccwf-ml-data/jigsaw/processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target'].apply(lambda x: 0 if x <= 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_TEXT = '/home/luis/ml-data/crawl-300d-2M.vec'\n",
    "GLOVE = '/home/luis/ml-data/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnCompatibleLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABLSTM(object):\n",
    "    def __init__(self, config):\n",
    "        self.max_len = config[\"max_len\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"]\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "        self.n_class = config[\"n_class\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n",
    "        self.label = tf.placeholder(tf.int32, [None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    def build_graph(self):\n",
    "        print(\"building graph\")\n",
    "        # Word embedding\n",
    "        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n",
    "                                     trainable=True)\n",
    "        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n",
    "\n",
    "        rnn_outputs, _ = bi_rnn(CudnnCompatibleLSTMCell(self.hidden_size),\n",
    "                                CudnnCompatibleLSTMCell(self.hidden_size),\n",
    "                                inputs=batch_embedded, dtype=tf.float32)\n",
    "\n",
    "        fw_outputs, bw_outputs = rnn_outputs\n",
    "\n",
    "        W = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n",
    "        H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
    "        M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
    "\n",
    "        self.alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n",
    "                                                        tf.reshape(W, [-1, 1])),\n",
    "                                              (-1, self.max_len)))  # batch_size x seq_len\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
    "                      tf.reshape(self.alpha, [-1, self.max_len, 1]))\n",
    "        r = tf.squeeze(r)\n",
    "        h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
    "\n",
    "        h_drop = tf.nn.dropout(h_star, self.keep_prob)\n",
    "\n",
    "        # Fully connected layerï¼ˆdense layer)\n",
    "        FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n",
    "        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n",
    "        y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
    "\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n",
    "\n",
    "        # prediction\n",
    "        self.prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
    "\n",
    "        # optimization\n",
    "        loss_to_minimize = self.loss\n",
    "        tvars = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n",
    "                                                       name='train_step')\n",
    "        print(\"graph built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def data_preprocessing_v2(train, test, max_len, max_words=50000):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(train)\n",
    "    train_idx = tokenizer.texts_to_sequences(train)\n",
    "    test_idx = tokenizer.texts_to_sequences(test)\n",
    "    train_padded = pad_sequences(train_idx, maxlen=max_len, padding='post', truncating='post')\n",
    "    test_padded = pad_sequences(test_idx, maxlen=max_len, padding='post', truncating='post')\n",
    "    # vocab size = len(word_docs) + 2  (<UNK>, <PAD>)\n",
    "    return train_padded, test_padded, max_words + 2\n",
    "\n",
    "def split_dataset(x_test, y_test, dev_ratio):\n",
    "    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n",
    "    test_size = len(x_test)\n",
    "    dev_size = (int)(test_size * dev_ratio)\n",
    "    x_dev = x_test[:dev_size]\n",
    "    x_test = x_test[dev_size:]\n",
    "    y_dev = y_test[:dev_size]\n",
    "    y_test = y_test[dev_size:]\n",
    "    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n",
    "\n",
    "def fill_feed_dict(data_X, data_Y, batch_size):\n",
    "    \"\"\"Generator to yield batches\"\"\"\n",
    "    # Shuffle data first.\n",
    "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
    "    # print(\"before shuffle: \", data_Y[:10])\n",
    "    # print(data_X.shape[0])\n",
    "    # perm = np.random.permutation(data_X.shape[0])\n",
    "    # data_X = data_X[perm]\n",
    "    # shuffled_Y = data_Y[perm]\n",
    "    # print(\"after shuffle: \", shuffled_Y[:10])\n",
    "    for idx in range(data_X.shape[0] // batch_size):\n",
    "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
    "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
    "        yield x_batch, y_batch\n",
    "        \n",
    "def make_train_feed_dict(model, batch):\n",
    "    \"\"\"make train feed dict for training\"\"\"\n",
    "    feed_dict = {model.x: batch[0],\n",
    "                 model.label: batch[1],\n",
    "                 model.keep_prob: .5}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def make_test_feed_dict(model, batch):\n",
    "    feed_dict = {model.x: batch[0],\n",
    "                 model.label: batch[1],\n",
    "                 model.keep_prob: 1.0}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def run_train_step(model, sess, batch):\n",
    "    feed_dict = make_train_feed_dict(model, batch)\n",
    "    to_return = {\n",
    "        'train_op': model.train_op,\n",
    "        'loss': model.loss,\n",
    "        'global_step': model.global_step,\n",
    "    }\n",
    "    return sess.run(to_return, feed_dict)\n",
    "\n",
    "\n",
    "def run_eval_step(model, sess, batch):\n",
    "    feed_dict = make_test_feed_dict(model, batch)\n",
    "    prediction = sess.run(model.prediction, feed_dict)\n",
    "    acc = np.sum(np.equal(prediction, batch[1])) / len(prediction)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_attn_weight(model, sess, batch):\n",
    "    feed_dict = make_train_feed_dict(model, batch)\n",
    "    return sess.run(model.alpha, feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, vocab_size = data_preprocessing_v2(train_df['processed_comment_text'], \n",
    "                                                        test_df['processed_comment_text'], 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size:  180487\n"
     ]
    }
   ],
   "source": [
    "# split dataset to test and dev\n",
    "x_train, x_dev, y_train, y_dev, dev_size, train_size = \\\n",
    "    split_dataset(x_train, y_train, 0.1)\n",
    "print(\"Validation Size: \", dev_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_len\": 600,\n",
    "    \"hidden_size\": 200,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_size\": 128,\n",
    "    \"n_class\": 2,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 512,\n",
    "    \"train_epoch\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building graph\n",
      "graph built successfully!\n",
      "validation accuracy: 0.954 \n",
      "Current loss is 0.13391779363155365\n",
      "validation accuracy: 0.963 \n",
      "Current loss is 0.0949307307600975\n",
      "validation accuracy: 0.964 \n",
      "Current loss is 0.09550566226243973\n",
      "validation accuracy: 0.964 \n",
      "Current loss is 0.12179908901453018\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=True)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        classifier = ABLSTM(config)\n",
    "        classifier.build_graph()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[\"batch_size\"]):\n",
    "            return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n",
    "            attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n",
    "            if return_dict['global_step'] % 500 == 0:\n",
    "                # Training loop. For each batch...\n",
    "                accuracy = 0.0\n",
    "                iter_cnt = 0\n",
    "                for x_val, y_val in fill_feed_dict(x_dev, y_dev, config[\"batch_size\"]):\n",
    "                    dev_batch = (x_val, y_val)\n",
    "                    dev_acc = run_eval_step(classifier, sess, dev_batch)\n",
    "                    accuracy += dev_acc\n",
    "                    iter_cnt += 1\n",
    "                accuracy /= iter_cnt\n",
    "                print(\"validation accuracy: %.3f \" % accuracy)\n",
    "                print(\"Current loss is {0}\".format(return_dict['loss']))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"run/model.ckpt\"))\n",
    "        save_path = saver.save(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, \"/tmp/model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
