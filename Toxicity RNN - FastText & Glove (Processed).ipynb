{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, CuDNNLSTM, Bidirectional, Input, GlobalAveragePooling1D\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D, concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('https://s3.amazonaws.com/ccwf-ml-data/jigsaw/processed_test.csv')\n",
    "train_df = pd.read_csv('https://s3.amazonaws.com/ccwf-ml-data/jigsaw/processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'] = train_df['target'].apply(lambda x: 0 if x <= 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_TEXT = 'crawl-300d-2M.vec'\n",
    "GLOVE = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(mat_type):\n",
    "    if mat_type == 'fasttext':\n",
    "        embeddings_index = {}\n",
    "        with codecs.open(FAST_TEXT, encoding='utf-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                values = line.rstrip().rsplit(' ')\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "        return embeddings_index\n",
    "    elif mat_type == 'glove':\n",
    "        tdf = pd.read_csv(GLOVE, sep=\" \", index_col=0, \n",
    "                               header=None, quoting=csv.QUOTE_NONE)\n",
    "        return dict(zip(tdf.index, tdf.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = get_embedding_matrix('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9cc7c469e8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['target'].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[(train_df['rating'] == 'rejected') & \n",
    "             (train_df['obscene'] > 0.1) & (train_df['severe_toxicity'] > 0), 'target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_train = train_df['processed_comment_text'].tolist()\n",
    "processed_docs_test = test_df['processed_comment_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  315829\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty sequences in training: 220\n"
     ]
    }
   ],
   "source": [
    "seq_lens = list(map(lambda x: len(x), word_seq_train))\n",
    "empty_sequences = len(list(filter(lambda x: x == 0, seq_lens)))\n",
    "print(\"Number of empty sequences in training:\", empty_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = max(seq_lens)\n",
    "VOCAB_SIZE = len(word_index)\n",
    "EMBEDDED_DIMS = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=MAX_SEQ_LEN)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 170436\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDED_DIMS))\n",
    "for word, i in word_index.items():\n",
    "    if i >= VOCAB_SIZE:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 540, 300)     94748700    words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 540, 300)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "RNN_Laye_1 (CuDNNLSTM)          (None, 540, 600)     2164800     spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "RNN_Layer_2 (CuDNNLSTM)         (None, 540, 600)     2884800     RNN_Laye_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pool (GlobalMaxPooling1D)   (None, 600)          0           RNN_Layer_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling1 (None, 600)          0           RNN_Layer_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1200)         0           max_pool[0][0]                   \n",
      "                                                                 avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1200)         1441200     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            1201        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 101,240,701\n",
      "Trainable params: 101,240,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_SEQ_LEN = 600\n",
    "\n",
    "words = Input(shape=(None,), name='words')\n",
    "emb = Embedding(VOCAB_SIZE, EMBEDDED_DIMS,\n",
    "          weights=[embedding_matrix], input_length=MAX_SEQ_LEN, name='embedding')\n",
    "X = emb(words)\n",
    "X = SpatialDropout1D(0.2)(X)\n",
    "X = CuDNNLSTM(LSTM_SEQ_LEN, return_sequences=True, name='RNN_Laye_1')(X)\n",
    "X = CuDNNLSTM(LSTM_SEQ_LEN, return_sequences=True, name='RNN_Layer_2')(X)\n",
    "hidden = concatenate([GlobalMaxPooling1D(name='max_pool')(X), GlobalAveragePooling1D(name='avg_pool')(X)])\n",
    "hidden = Dense(LSTM_SEQ_LEN*2, activation='relu')(hidden)\n",
    "result = Dense(1, activation='sigmoid')(hidden)\n",
    "model = Model(inputs=words, outputs=result)\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1759752 samples, validate on 45122 samples\n",
      "Epoch 1/16\n",
      "  37120/1759752 [..............................] - ETA: 1:06:05 - loss: 0.1947 - acc: 0.9353"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=256, epochs=16, \n",
    "                 validation_split=0.025, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a submission\n",
    "submission_df = pd.DataFrame(columns=['id'])\n",
    "submission_df['id'] = test_df['id'].values \n",
    "submission_df['prediction'] = y_test \n",
    "submission_df.to_csv(\"./rnn_fasttext_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "plt.title('RNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
    "plt.title('RNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
